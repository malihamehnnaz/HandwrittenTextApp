import numpy as np
import pandas as pd
import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
import os
import random 
import cv2
import pickle
import random
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelBinarizer
from keras.utils import np_utils
from keras.models import Sequential
from keras import optimizers
from sklearn.preprocessing import LabelBinarizer
from keras import backend as K
from keras.layers import Dense, Activation, Flatten, Dense,MaxPooling2D, Dropout,Conv2D, BatchNormalization
from pickle import dump

dir = r"C:\Users\HP\Desktop\archive (3)\dataset\Train\Capi"
dir2=r"C:\Users\HP\Desktop\archive (3)\dataset\Train\Number"
dir3=r"C:\Users\HP\Desktop\archive (3)\dataset\Train\Small"
train_data = []
img_size = 32
for i in os.listdir(dir):
    sub_directory = os.path.join(dir,i)
    print(sub_directory)
    for j in os.listdir(sub_directory):
        img = cv2.imread(os.path.join(sub_directory,j),0)
        img = cv2.resize(img,(img_size,img_size))
        train_data.append([img,i])
        
for i in os.listdir(dir2):
    sub_directory = os.path.join(dir2,i)
    print(sub_directory)
    for j in os.listdir(sub_directory):
        img = cv2.imread(os.path.join(sub_directory,j),0)
        img = cv2.resize(img,(img_size,img_size))
        train_data.append([img,i])
        
for i in os.listdir(dir3):
    sub_directory = os.path.join(dir3,i)
    print(sub_directory)
    for j in os.listdir(sub_directory):
        img = cv2.imread(os.path.join(sub_directory,j),0)
        img = cv2.resize(img,(img_size,img_size))
        train_data.append([img,i])

val_dir = r"C:\Users\HP\Desktop\archive (3)\dataset\Validation\Capital"
val_dir2 = r"C:\Users\HP\Desktop\archive (3)\dataset\Validation\Number"
val_dir3 = r"C:\Users\HP\Desktop\archive (3)\dataset\Validation\Small"
val_data = []
img_size = 32
for i in os.listdir(val_dir):
    sub_directory = os.path.join(val_dir,i)
    print(sub_directory)
    for j in os.listdir(sub_directory):
        img = cv2.imread(os.path.join(sub_directory,j),0)
        img = cv2.resize(img,(img_size,img_size))
        val_data.append([img,i])
        
for i in os.listdir(val_dir2):
    sub_directory = os.path.join(val_dir2,i)
    print(sub_directory)
    for j in os.listdir(sub_directory):
        img = cv2.imread(os.path.join(sub_directory,j),0)
        img = cv2.resize(img,(img_size,img_size))
        val_data.append([img,i])
        
for i in os.listdir(val_dir3):
    sub_directory = os.path.join(val_dir3,i)
    print(sub_directory)
    for j in os.listdir(sub_directory):
        img = cv2.imread(os.path.join(sub_directory,j),0)
        img = cv2.resize(img,(img_size,img_size))
        val_data.append([img,i])len(train_data)

len(val_data)

random.shuffle(train_data)

random.shuffle(val_data)

train_X = []
train_Y = []
for features,label in train_data:
    train_X.append(features)
    train_Y.append(label) 

with open(r"someobjectE.pickle", "wb") as output_file:
    pickle.dump(train_Y, output_file)
    
with open(r"someobjectE.pickle", "rb") as input_file:
    e = pickle.load(input_file)

val_X = []
val_Y = []
for features,label in val_data:
    val_X.append(features)
    val_Y.append(label)
#print(val_Y)
    
with open(r"someobjectF.pickle", "wb") as output_file:
    pickle.dump(val_Y, output_file)
    
with open(r"someobjectF.pickle", "rb") as input_file:
    f = pickle.load(input_file)

LB = LabelBinarizer()
train_Y = LB.fit_transform(train_Y)
val_Y = LB.fit_transform(val_Y)
train_X = np.array(train_X)/255.0
train_X = train_X.reshape(-1,32,32,1)
train_Y = np.array(train_Y)
val_X = np.array(val_X)/255.0
val_X = val_X.reshape(-1,32,32,1)
val_Y = np.array(val_Y)
print(train_X.shape,val_X.shape)

print(train_Y.shape,val_Y.shape)

model = tf.keras.Sequential([
  layers.Conv2D(64, 3, padding='same', activation='relu',input_shape=(32,32,1)),
  layers.MaxPooling2D(),
  layers.Conv2D(128, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(256, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),  
  layers.Dropout(0.2),    
  layers.Dense(128, activation='relu'),
  layers.BatchNormalization(),
  layers.Dense(80, activation='relu'),
  layers.Dense(62, activation='softmax')
])

model.compile(loss='categorical_crossentropy', optimizer="adam",metrics=['accuracy'])
history = model.fit(train_X,train_Y, epochs=40,batch_size=20,shuffle=True, validation_data = (val_X, val_Y),  verbose=1)


model.save("all_model.h5")
model.summary()